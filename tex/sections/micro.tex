\section{Mikroprozessoren}\label{2}

Für das Verständnis eines DSPs ist das Verständnis der zu Grunde liegenden Hardware unumgänglich. Hierzu wird zunächst in Kapitel \ref{2.1} der Begriff 'Echtzeit' erklärt. In der praktischen Anwendung ist die Signalverarbeitung ein Echtzeit-Problem, weswegen dieser Begriff eine wichtige Grundlage bildet. Des weiteren führt dieses Kapitel noch andere ebenfalls wichtige Begriffe ein, beispielsweise 'harte Echtzeit' oder auch 'Deadlines'.\\
Kapitel \ref{2.2} befasst sich mit der Bauweise von Speicherelementen. Dazu zählen 'Random-Access-Memory' (RAM), 'Read-Only-Memory' (ROM), 'Flash'-Speicher (EEPROM) sowie Caches. Insbesondere wird auf Vor- und Nachteile dieser Optionen eingegangen.\\
Bestandteile des Kapitels \ref{2.3} sind verschiedene Architekturen, wobei Harvard sowie Von-Neumann eine tragende Rolle spielen.
Im Folgenden befasst sich Kapitel \ref{2.4} mit weitergehender Architektur, welche entscheidende Vorteile für die Verarbeitung von Signalen bietet. Hierzu zählen Pipelines und Multicore Architekturen.\\
Für die Verarbeitung von Daten sind grundlegende Befehle notwendig. In welcher Form und in welchem Umfang diese elementaren Befehle vorliegen wird in Kapitel \ref{2.5} betrachtet. Besondere Aufmerksamkeit wird der praktischen Anwendung in der Signalverarbeitung gewidmet.\\
Kapitel \ref{2.6} behandelt das Thema 'Scheduling', also der zeitliche Ablauf verschiedener Prozesse. Hierbei werden Verfahren wie 'Round-Robin', 'First-Come-First-Serve' sowie 'Earliest Deadline First' analysiert.\\
Schließlich werden weitere Optionen wie 'Mailboxes' sowie Gefahren, beispielsweise 'Deadlocks' in Kapitel \ref{2.7} aufgezählt. Besonders wird auf Themen eingegangen, die einen Mikroprozessor in der Signalverarbeitung stören könnten.\\
\newpage

\subsection{Grundlegende Begriffe}\label{2.1}
Um den Begriff der Echtzeit zu verstehen, ist es notwendig, zunächst die Bezeichnung 'System' zu definieren:
\begin{definition}{System}\\
Ein System beschreibt die Zuordnung von Eingabevektoren auf Ausgabevektoren.\cite[p. 3]{RTS}
\end{definition}
Dabei müssen weder alle denkbaren Eingaben angenommen, noch alle denkbaren Ausgaben ausgegeben werden können. Beispielsweise besteht kein Grund dazu, dass ein Geldautomat auf Audiosignale reagiert. Die Tastatur und die Karteneingabe beschränkt somit den Eingaberaum auf eine endlich Anzahl an Eingaben.\\
Andere Systeme besitzen lediglich Sensoren als Eingabegeräte, beispielsweise einen Wärmesensor. Bei diesem besteht der Eingaberaum aus allen möglichen Ausgabewerten des Sensors.\\
\newline
Der Ausgaberaum besteht aus jeder möglichen Repräsentation der Ausgabe des Systems. Im Falle des Geldautomaten wäre es das Display, die Rückgabe der Karte sowie die Ausgabe der Geldscheine.\\
Doch die Ausgabe muss von außen nicht direkt sichtbar sein. Ebenso können Daten den Ausgaberaum beschreiben. Im Beispiel des Wärmesensors könnte das System mit einer Heizung verbunden sein, die mithilfe der von dem System ausgegebenen Werte die Temperatur reguliert.\\
\newline
'Echtzeit' begrenzt den Zeitraum, in dem die Ausgabe auf eine bestimmte Eingabe folgen muss.
\begin{definition}{Echtzeit}\\
Ein Echtzeit-System ist ein Computer-System, welches bestimmte Antwortzeiten('Deadlines') einhalten muss. Andernfalls ist mit gravierenden Folgen, darunter auch einem Ausfall zu rechnen.\cite[p. 5]{RTS}
\end{definition}
Zur Verdeutlichung werden die vorangegangenen Beispiele erneut genutzt.\\
Man stelle sich vor, der Geldautomat würde das Geld erst nach einigen Minuten ausgeben. Dies wäre nicht gefährlich, jedoch unangenehm und würde die Zuverlässigkeit erheblich einschränken.\\
Anders sieht es im Falle des Temperaturreglers aus. Sensor und Heizung seien nun mit einem Aquarium verbunden. Die Ausgabe des Systems, die Heizung auszuschalten verzögert sich nun um einige Stunden. Bis die eigentliche Regulierung erfolgt ist das Wasser wahrscheinlich schon bei weitem zu heiß. Die Auswirkungen sind hier weitaus drastischer als bei dem Geldautomaten.\\
\newline

\begin{definition}{Deadline}\\
Als Deadline wird eine einer Aufgabe zugeordneter Zeitspanne bezeichnet, welche die größtmögliche Zeit zum Beenden dieser Aufgabe angibt.
\end{definition}

Es ist offensichtlich, dass nicht alle denkbaren Systeme in Echtzeit agieren müssen. Bei vielen Anwendungen ist diese Bedingung jedoch unumgänglich, wie auch bei der Signalverarbeitung. Sollte die Verarbeitung einer Zeitreihe länger als die Dauer der Zeitreihe benötigen, so kann dies nicht als Echtzeitsystem bezeichnet werden.\\

\begin{bew}{Deadline}\\
Sei $t_i$ die Dauer einer Zeitreihe $i$. Entsprend sei $p_i = t_i + d_i$ die Verarbeitungszeit der Zeitreihe, bestehend aus der Länge sowie einem Delay $d_i$.\\
Sei $t_l$ eine fixe Zeit, die die maximal zugelassene Verzögerung einer Zeitreihe angibt, also die Deadline.\\
Werden nun mehrere Eingaben nacheinander berechnet, so summiert sich der Delay auf:\\
$d = \sum_{i=1}^N d_i$\\
Angenommen, der Delay sei konstant. So wird nach $\frac{t_l}{d_i}$ Eingaben der Delay die Deadline überschreiten. Dies gilt für jede endliche Deadline.
\end{bew}
Jedoch muss die Deadline nicht der Länge der Zeitreihe entsprechen. Hört man Musik, so möchte man nicht, dass die Ausgabe Sekunden nach dem eigentlichen Abspielen erfolgt, sondern ohne merkliche Verzögerung. Auch von der Eingabe mittels einer Tastatur bis zur Visualisierung des Zeichens auf dem Bildschirm sollte nicht mehr als wenige Zehntel einer Sekunde vergehen.

Die tiefere Bedeutung des Begriffes 'Echtzeit' wird deutlich, wenn verschiedene Unterarten definiert werden. Im Folgenden werden drei Kategorien vorgestellt: Weich, hart sowie fest.\\
\begin{definition}{Weiche Echtzeit}\\
Ein weiches Echtzeitsystem zeichnet sich dadurch aus, dass auch viele verpasste Deadlines nicht zu einem Versagen des Systems führen können, sondern lediglich die Performance beeinträchtigen können.
\end{definition}
Im Gegensatz hierzu stehen die harten Echtzeitsysteme:\\
\begin{definition}{Harte Echtzeit}\\
Ein hartes Echtzeitsystem zeichnet sich dadurch aus, dass bereits eine verpasste Deadline zu einem kompletten Systemversagen führen kann.
\end{definition}
Das wohl beste Beispiel ist die Vorstellung, dass Sensoren die Leistung eines Atomkraftwerkes überwachen sollen. Steht nun eine Überlastung bevor, so muss das System binnen weniger Augenblicke reagieren - sollte diese Deadline nicht eingehalten werden, so kann dies eine Katastrophe zur Folge haben.\\
Auf der anderen Seite steht der Geldautomat, dessen verpasste Deadlines auch in großer Zahl weder Systemversagen- noch Katastrophen mit sich führen, jedoch massive Performanceverluste bedeuten.\\
\begin{definition}{Feste Echtzeit}\\
Ein festes Echtzeitsystem zeichnet sich dadurch aus, dass wenige verpasste Deadlines lediglich die Performance beeinträchtigen, eine größere Zahl jedoch zu Systemversagen führen könnte.
\end{definition}
Es existiert keine eindeutige Bestimmungsvorschrift für die Art eines Systems. Die Grenzen sind nicht fest definiert und so kann ein System je nach Betrachter als 'fest' oder 'hart' bezeichnet werden.\\
Jedoch ist diese dritte Klasse sinnig, um nicht nur zwischen einem sehr robusten System und einem,
dessen Deadlines um jeden Preis eingehalten werden müssen unterscheiden zu können.\\
Ein weiterer wichtiger Begriff, welcher vor allem zur Abschätzung der Zuverlässigkeit eines Systems sowie des Verhaltens von bestimmten Aufgaben genutzt wird lautet 'Determinismus'.\\
\begin{definition}{Deterministisches System}\\
Ein deterministisches System zeichnet sich dadurch aus, dass für jeden möglichen Zustand und jeden möglichen Eingabevektor der Folgezustand sowie Ausgaben bestimmt werden können.
\end{definition}
Ist dies für ein System der Fall, so lässt sich sein gesamtes Verhalten bestimmen und jede mögliche Verletzung von Deadlines ist im Vorfeld bestimmbar.\\
Bei einem nicht-deterministischen System besteht die Herausforderung darin, Laufzeiten realistisch abzuschätzen. Hier wird in best- und worst-case Zeiten argumentiert, also minimale- bzw. maximale Zeitspannen für die Beendigung einer Aufgabe.\\
Wird immer die bestmögliche Zeit angenommen, so wird dies bei Nichtdeterminismus unweigerlich zu verpassten Deadlines führen. Im anderen Fall, also der konstanten Annahme von worst-case Zeiten werden Deadlines immer eingehalten werden, jedoch wird das System nicht optimal ausgenutzt. Diese Zeiten abschätzen zu können führt also zu einer Optimierung der Effizienz.\\
Im Folgenden wird gezeigt, dass viele Optimierungsmethoden Nichtdeterminismus mit sich führen.\\
Ein weiterer wichtiger Begriff, der am Ende des Kapitels eine tragende Rolle spielt ist der 'Interrupt'.
\begin{definition}{Interrupt}\\
Als Interrupt wird ein Signal bezeichnet, welche die aktuelle Aufgabe des Prozessors unterbricht, um eine mit dem Interrupt verknüpfte Aufgabe auszuführen.
\end{definition}
Als Beispiel für einen Interrupt kann ein externes Eingabegerät genommen werden. Wird bei einer Tastatur eine Taste gedrückt, so wird die aktuelle Aufgabe des Prozessors unterbrochen, die Eingabe entgegengenommen, verarbeitet und anschließend die zuvor behandelte Aufgabe weiter behandelt. Dabei wird der aktuelle Zustand und die Variablen der Aufgabe auf den 'Stack' gelegt, um sie anschließend in eben dieser Reihenfolge auslesen zu können. Somit muss die Aufgabe nicht von Neuem begonnen werden.
\subsection{Speicher}\label{2.2}
Es lassen sich verschiedene Arten von Speicher angeben. Dabei spielen nicht nur Schreib- und Lesezeiten, Kosten und die Flächennutzung eine Rolle, sondern auch Energieverbrauch und die Möglichkeit des Schreibens neuer Daten. Hierzu werden im Folgenden auf die Arten RAM, ROM sowie Flash-Speicher eingegangen. Ebenfalls wird der Unterschied zwischen dynamischem und statischem RAM deutlich gemacht.

\subsubsection{Random Access Memory}
Unter Random Access Memory (RAM) versteht man eine unbeständige Art der Datenspeicherung. Diese Art von Speicher lässt sich in der Regel beliebig oft neu beschreiben, löschen und auslesen. All diese Vorgänge sind schneller als bei anderen Arten von Speicher, weshalb jede Form eines modernen Rechners eine Form von RAM besitzt, meist in der Regel eines Caches, der im folgenden noch beschrieben wird.\\
Durch die unbeständige Art der Datenspeicherung eignet sich RAM-Speicher jedoch nicht für persistente Speicherung. Sobald die Stromversorgung unterbrochen wird, werden die Speicherzellen geleert und die Daten somit verworfen.

\subsubsection{Read-Only Memory}
Im Gegensatz dazu steht Read-Only Memory, auch ROM. Wie der Name bereits andeutet, eignet sich dieser nicht zur Änderung der gespeicherten Daten. Es ist möglich, den Speicher zu leeren und zu beschreiben, jedoch ist die maximale Anzahl dieser Vorgänge limitiert und um einiges langsamer als beim RAM. Dies ist jedoch nicht bei jeder Form von ROM möglich. Mask ROM beispielsweile lässt sich nur bei der Herstellung beschreiben. Ebenso ist das Auslesen der Daten langsamer.\\
Die Speicherung ist persistenter Natur, weswegen ROM in der Regel fest verdrahtet Firmware für ein Gerät bereit stellt, welche nicht alternierbar ist. Ein weiterer Vorteil ist der weitaus geringere Preis.

\subsubsection{Flash}
Flash-Speicher, auch Flash-EEPROM (Electrically Erasable Programmable Read-only Memory) ist eine erweiterte Form des normalen ROMs. Im Gegensatz zu EEPROM können einzelne Bytes nicht überschrieben werden.\\
Dank der Natur des ROM-Speichers sind Daten jedoch persistent gespeichert. Ebenso ist der Energieverbrauch verhältnismäßig gering, und die Kosten sind ebenfalls minimal. Somit bietet der Flash-Speicher eine robuste, kosteneffiziente Lösung für eingebettete Systeme.

\subsubsection{Cache}
Der Cache bezeichnet eine Speicherschicht zwischen dem Prozessor und dem Hauptspeicher. Zweck dieser Schicht ist es, Daten temporär zu speichern, um diese bei Anfrage schneller als der eigentliche Hauptspeicher zur Verfügung stellen zu können. Es ist ersichtlich, dass ein Cache in der Regel mittels Random-Access Memory realisiert wird.\\
Der Nachteil besteht darin, dass der Zugriff auf nicht im Cache verfügbare Daten länger dauert als bei einer Realisierung ohne Cache. Zunächst werden die Daten angefordert, anschließend wird überprüft, ob diese Daten im Cache vorliegen. Diese Zeitspanne würde bei direktem Zugriff auf den Hauptspeicher entfallen. Somit ergibt sich das Problem, eben jene Wahrscheinlichkeit, Daten im Cache vorzufinden zu maximieren.\\
Ein weiterer Nachteil folgt aus der nicht-deterministischen Zugriffszeit auf Daten, da man im Vorfeld nicht abschätzen kann, ob sich ein bestimmtes Datum im Cache befindet.

\subsubsection{Locality of reference}
Der Begriff 'Locality of reference' bezeichnet das Phänomen, bei dem entweder die gleiche Speicherstelle, oder der gleiche Datenwert oft genutzt wird. Diese werden respektiv mit räumlicher- und zeitliche Lokalität bezeichnet. Im Kontext des Caches werden häufig angeforderte Daten, oder auch zusammenhängende Datenblöcke wie ein sequentiell ausgeführtes Programm abgespeichert, um diese in Zukunft mit sehr geringer Zugriffszeit anzufordern. Werden vielen Daten häufig angefordert, oder liegt der Code sehr oft in aufeinanderfolgenden Speicherzellen vor, so liegt eine hohe locality of reference vor. Je höher dieser Wert ist, desto eher liegen angeforderte Daten im Cache, und desto geringer wird die durchschnittliche Zugriffszeit.

\subsubsection{Anwendung bei einem DSP}
In der Regel wird bei einem DSP sowohl RAM- als auch ROM genutzt. Am Beispiel \cite{TI} sieht man die Nutzung eines Read-Only Memory Blocks als Speicher für den Bootloader, sowie eine $8k$Bytes RAM-Speichereinheit als Datenspeicher. Dieser Speicher reicht aus, da ein- und ausgehende Daten nicht persistent auf dem DSP gespeichert werden, sondern lediglich zur Berechnung vorliegen.\\
Des weiteren lässt sich erkennen, dass ein $512$Byte Instruction Cache vorliegt. In der Regel werden bei einem DSP wenige Befehle benötigt, die für alle eingehenden Daten genutzt werden. Somit ist es sinnig, diese Befehle auf einem Cache zwischenzuspeichern, um diese of auftretende Zugriffszeit zu minimieren.\\
Für die Daten wird jedoch kein Cache genutzt, da diese in der Regel nur ein einziges Mal betrachtet werden. Dieses Element würde somit lediglich Nichtdeterminismus- sowie langsamere Zugriffszeiten mit sich führen. 


\subsection{Harvard/Von-Neumann Architektur}\label{2.3}
Bei der Gestaltung eines Computersystems existieren zwei vorherrschende Lösungen - von-Neumann und Harvard. Der Unterschied hier liegt in der Art der Speicherung von Daten und Instruktionen.

\subsubsection{Central Processing Unit}
Das wichtigste Element eines Computersystems ist die CPU (central processing unit). Diese besteht aus einer Control Unit, einem Datapath sowie diversen Bussen. Die Control Unit besitzt ein Programm Counter Register und ein Instruction Register. Die hier ausgehenden Befehle werden zum einen über den Internen Bus an den Datapath weitergeleitet, und zum anderen besteht eine Verbindung zum Rest des Systems, um andere Elemente direkt ansprechen zu können.\\
Der Datapath besteht aus einem MAR (memory address register) sowie einer ALU (arithmetic-logic unit). Somit können eingehende Befehle aus der Control Unit behandelt werden. Mittels des MARs können Daten angefordert werden, welche in der ALU verarbeitet und erneut abgespeichert werden. Dazu besteht auch hier eine Verbindung zu dem restlichen System.\\

\subsubsection{Von-Neumann}
Der Grundlegende Aufbau besteht aus einer CPU, einem Speicher und einem Systembus. 
Der Bus setzt sich aus einem Daten- und einem Adressen-Bus zusammen, und verbindet CPU mit dem Speicher. Die Daten einer bestimmten Adresse werden von der CPU angefordert, kurz darauf von dem Speicher auf dem Datenbus bereit gestellt und von dem Datapath verarbeitet.\\
Jedoch ist diese Architektur theoretischer Natur. Sie ist realisierbar, jedoch fehlt jede Möglichkeit der Ein- oder Ausgabe von Daten, und ist somit nicht nutzbar.\\
Die erweiterte von-Neumann Architektur bietet eine Lösung, indem sie den Systembus zusätzlich mit einem I/O-Gerät (Input/Output) verbindet. Durch ein weiteres Kontroll-Signal wird festgelegt, ob Speicher- oder I/O angesprochen wird. 

\subsubsection{Harvard}
Im Gegensatz zu der Von-Neumann Architektur besitzt die Harvard-Achitektur zwei Speicher, einen Befehls- und einen Datenspeicher. Zusätzlich existieren hier zwei Busse, von denen jeder einen Daten- und einen Adressbus beinhaltet. Somit können sowohl Befehle als auch Daten unabhängig voneinander angefordert und geladen werden. Zusätzlich können die einzelne Busse unterschiedliche Breiten haben, wodurch das System besser an den jeweiligen Anwendungsfall angepasst werden kann.\\
Ein Nachteil ist, dass die Verwaltung von mehreren Adress-Bereichen komplexer sein kann, und somit eine weitere Herausforderung darstellt.

\subsubsection{Anwendung bei einem DSP}
Für die Realisierung eines DSPs wird in der Regel eine Harvard-Architektur verwendet. Hier können mehrere Datenspeicher und dazugehörige Busse existieren, die die Verarbeitung von einer großen Menge an Daten ermöglichen. \cite{TI_c55x} beispielsweise besitzt drei Lese- und zwei Schreib-Busse für den Datenspeicher, jedoch nur einen Adress-Bus. Zudem beträgt die Breite der Datenbusse jeweils 16 Bit, während der Befehlsbus eine Breite von 32 Bit besitzt.\\
Der Nachteil an der Von-Neumann Architektur ist in diesem Fall, dass Daten und Befehle über den gleichen Bus übertragen werden.  Dies impliziert, dass nur eine Breite verwendet wird, in Fall von \cite{TI_c55x} also 32 Bit, was für die Daten zu viel wäre.

\subsection{Weitergehende Architektur}\label{2.4}
Zur Optimierung dieser Systeme sind eine Mehrzahl von Möglichkeiten realisiert worden. Zwar besitzt jede dieser Methoden Nachteile, jedoch überwiegen die Vorzüge bei der Anwendung in einem passenden System. Die Herausforderung hier besteht darin, ein System einzuschätzen und notwendige- sowie überflüssige Optimierungen voneinander abzugrenzen.

\subsubsection{FDLES}
Das Apronym 'FDLES' bezeichnet den Prozess eines Befehls, der von der CPU verarbeitet wird. Dieser steht für 'Fetch, Decode, Load, Execute, Store'.

\begin{itemize}
\item Fetch Instruction - Lade die Operation aus dem Befehlsspeicher
\item Decode Instruction - Entschlüssele die Operation
\item Load Operand - Lade die Operanden aus dem Datenspeicher
\item Execute ALU function - Führe die Operation mithilfe der Operanden auf der ALU durch
\item Store results - Speichere das Ergebnis unter einer gegebenen Adresse ab 
\end{itemize}
Bei einer simplen serielle Verarbeitung der Befehle werden für jeden Befehl fünf Zyklen benötigt. Bei einer benötigten Anzahl von $n$ Befehlen ergeben sind $5\cdot n$ Befehle, und bei einer Laufzeit von $t$ für jeden Zyklus eine Gesamtzeit von $5\cdot n\cdot t$.
\subsubsection{Pipelines}
\begin{figure}
\centering
\includegraphics[scale=0.5]{images/pipeline.png}
\captionsource{Pipelined instruction processing in a five-stage pipeline.}{\cite[p. 45]{RTS}}
\label{pipeline}
\end{figure}
Der größte Nachteil an der seriellen Ausführung dieser Phasen besteht darin, dass zu jedem Zeitpunkt vier dieser fünf Elemente untätig ist. Wird beispielsweise gerade ein Befehl geladen, so sind sowohl Decoder, Datenbus als auch ALU untätig.\\
Somit wurde der Begriff der 'Pipeline' eingeführt, um dieses Problem zu umgehen. Hierbei werden zu jedem Zeitpunkt fünf Befehle zugleich verarbeitet, was in Abb. \ref{pipeline} sichtbar ist. Damit ergäbe sich eine Laufzeit von $(n+4) \cdot t$, da nach Beginn des letzten Befehls noch vier Zyklen vergehen, um diesen zu beenden.\\
Hier stellt sich jedoch die Frage, welcher Befehl als nächstes geladen wird. Beim seriellen Verfahren wird ein Ergebnis berechnet, gespeichert und auf dessen Grundlage der nächste Befehl gewählt. Bei einem parallelen Verfahren wird davon ausgegangen, dass die Befehle in sequentieller Form vorliegen und somit nach dem Befehl $n$ der Befehl $n+1$ benötigt wird. Solange dies zutrifft lässt sich eine Zeit von $(n+4) \cdot t$ einhalten. Wird jedoch ein 'falscher' Befehl geladen, so muss die Pipeline 'geflushed', also von allen Befehlen gereinigt werden. Dies erzeugt zusätzliche Laufzeit. Wird eine Pipeline in einem System angewendet, dessen Befehle nicht darauf angepasst worden sind und nicht in sequentieller Form vorliegen, so kann die Laufzeit die eines seriellen Systems übertreffen.\\
Somit lässt sich konkludieren, dass sich mithilfe von Pipelines massive Performance-Verbesserungen erzielen lassen, solange das System hierfür entsprechend angepasst wird.

\subsubsection{Multi-Core}
Die Taktfrequenz eines Prozessors ist nicht beliebig erhöhbar. Bei Frequenzen im geringen Gigahertz-Bereich sorgt die entstehende Abwärme dafür, dass sich die Hardware bereits nach kurzer Zeit selber zerstören würde. Ebenfalls wird die Synchronisation der Daten in solchen Frequenzbereichen kompliziert. Um die Leistungsfähigkeit eines Prozessors weiter erhöhen zu können sind Mehrkernprozessoren eingeführt worden. Jeder dieser Kerne besitzt einen eigenen 'Instuction\& Data-Cache', welche mit einem gemeinsamen 'Internal Cache Bus' verbunden sind. Dieser wiederum ist mit einem 'Common Instruction\& Data-Cache' verbunden, welcher die Schnittstelle zu dem 'System Bus' darstellt.\\
Theoretisch ist somit eine Vervielfachung des Durchsatzes an Aufgaben möglich, welche proportional zu der Anzahl an Kernen ist. Voraussetzung hierfür ist eine optimale Verteilung der Aufgaben auf die Kerne, sowie eine parallele Verarbeitung, Anforderung und Speicherung von Daten. Die Realisierung hiervon ist jedoch nicht trivial. Zudem führt ein Mehrkernprozessor gleichzeitig auch starken Nichtdeterminismus ein, welcher seinen Ursprung in der Verteilung der Aufgaben, Pipelines sowie den individuellen und dem gemeinsamen Cache findet. Des weiteren ist es wichtig, die Kommunikation zwischen den einzelnen Kernen zu gewährleisten, welche zuverlässig und, vor allem anderen, schnell möglich sein muss.\\
Bei gründlicher Planung und Gestaltung eines Mehrkernprozessors können enorme Fortschritte in der 'Geschwindigkeit' eines Prozessors erzielt werden, ohne sich um Hitzeentwicklung oder Gatterlaufzeiten Gedanken zu machen. Jedoch bieten sich auch weitaus mehr Aspekte, die anfällig für Fehler sind.

\subsection{CISC/RISC}\label{2.5}
Eine weitere notwendige Entscheidung bei der Gestaltung eines Systems wird von der Art der Befehlsatzes gestellt. Es gibt zwei vorherrschende Optionen - 'Complex Instruction Set Computers' (CISC) sowie 'Reduces Instruction Set Computers' (RISC). Diese unterscheiden sich in einer Vielzahl an Aspekten, beispielsweise die Anzahl der Befehle, deren Laufzeit und die Komplexität.

\subsubsection{Complex Instruction Set Computers}
Diese Art eines Befehlssatzes bietet eine Vielzahl komplexer Befehle, deren Ziele die Minimalisierung der Laufzeit, Reduktion des Speicherverbrauchs sowie vereinfachte Handhabung sind. Erreicht wird dies durch die Implementierung von frequentiert genutzten Anweisungen.\\
Als Beispiel diene folgender Befehl:\\

MULT $\$A, \$B$\\
\newline
Dieser Befehl sagt aus, dass die Werte der Register $A$ und $B$ geladen, multipliziert und in dem Register $A$ gespeichert werden. Dabei bezeichnet $\$A$ das Register mit der Nummer A. Die äquivalente Form eines 'RISC' sähe wie folgt aus:\\
\newpage
LOAD $x$, $\$A$\\ 
LOAD $y$, $\$B$\\
PROD $x$, $y$\\
STORE $\$A$, $x$\\
\newline
Hier werden die Register $A$ und $B$ in die Variablen $x$ und $y$ geladen, multipliziert, in der Variable $x$ zwischengespeichert und anschließend in dem Register $A$ gelagert. Es ist offensichtlich, dass die 'CISC'-Variante weitaus einfacher zu verwenden ist. Ebenfalls wird die Größe dieses Programms um 75\% reduziert, und somit werde weniger Daten von dem Hauptspeicher gelesen. Dies reduziert zusätzlich die Laufzeit des Codes.

\subsubsection{Reduces Instruction Set Computers}
Am vorangehenden Beispiel wird deutlich, dass sich 'RISC' auf elementare Befehle beschränkt. Der wohl wichtigste Aspekt ist, dass jeder Befehl genau einen Taktzyklus dauert. Durch diese einheitliche Laufzeit werden Pipelines und Parallelismus effizienter nutzbar. Der größte hierdurch entstehende Nutzen folgt daraus, dass ein Interrupt nach jedem Taktzyklus möglich ist. Ebenfalls ist somit kein Fokus auf bestimmte Befehle gelegt, sondern jede mögliche Kombination dieser elementaren Befehle ist denkbar. Ein weiterer Vorteil besteht in der reduzierten Speichernutzung dieser Befehle. Durch eine minimale Anzahl an sehr kompakten Befehlen wird nun ein Bruchteil des von einer 'CISC'-Variante genutzten Speichers benötigt.

\subsubsection{Anwendung bei einem DSP}
In der Regel wird ein 'DSP' mittels eines 'CISC' realisiert. Durch die schnelle Reaktion auf einen Interrupt sowie die geringe Anzahl an benötigten Befehlen eines 'DPS's bietet sich diese Wahl an. Die Implementierung von 'MAC'-Befehlen ist jedoch nicht optional und stellt den wichtigsten Aspekt dieses Systems dar. Der Begriff 'MAC' steht dabei für 'Multiplication-Accumulation' und erfordert für diese Operation genau einen Taktzyklus und ist einer der wichtigsten Befehle für die Verarbeitung von Signalen.

\subsection{Scheduling}\label{2.6}
Jedes nicht triviale System besitzt eine Vielzahl an möglichen Aufgaben, welche in der Regel eine Deadline besitzen. Somit stellt es eine wichtige Herausforderung dar, Aufgaben in der optimalen Reihenfolge abzuarbeiten, um nach Möglichkeit keine Deadline zu verpassen. Zunächst werden triviale Lösungen vorgestellt, und anschließend Überlegungen, welche Deadlines, verschiedene Zustände der Aufgaben sowie deren Frequenz berücksichtigen.

\subsubsection{Polled Loop with Delay}

\begin{algorithm}
\captionsource{Polled Loop}{\cite[p. 83]{RTS}}
\label{sced1}
\begin{algorithmic}[1]

\BState \textit{While(True)}\\
\{
\If {$flag$} \{
\State $process\_event();$
\State $pause(21);$
\State $flag=0$;
\}
\EndIf
\}

\end{algorithmic}
\end{algorithm}

Wird ein Event in unbestimmten Zeitabständen erwartet, so kann die Polled Loop genutzt werden. Sobald das Event auftritt wird die Flag gesetzt. Dies führt innerhalb der Loop dazu, dass das Event behandelt wird und die Flag zurückgesetzt wird. Der Grund für die Pause von $21m$s besteht darin, dass gewisse Events während oder nach deren Ausführung ein unvorhersehbares Verhalten besitzen können. Kann man davon ausgehen, dass dieses Event eine minimale zeitliche Differenz zwischen den einzelnen Ausführungen hat, so kann solch eine Pause eine Robustheit des Systems gegenüber solch eines Verhaltens bieten.\\
Allerdings ist ersichtlich, dass diese Schleife weder spezielle Features bietet, noch die Verarbeitung von mehr als einem Event unterstützt.\\
Abhilfe für das zweite Problem würde die Einführung von verschiedenen Flags bieten, welche innerhalb der Schleife nacheinander überprüft werden und mit einem eigenen Event verknüpft sind. Dies würde bei einer Vielzahl von Events jedoch dazu führen, dass im schlimmsten Fall jede Flag überprüft wird, ehe die gesetzte Flag bemerkt wird. Der dadurch entstehende Delay kann zu Problemen führen.

\subsubsection{Cyclic Code Structure}
\begin{algorithm}
\captionsource{Cyclid Code}{\cite[p. 84]{RTS}}
\label{sced2}
\begin{algorithmic}[1]

\BState \textit{While(True)}\\
\{
\State $task\_1();$
\State $task\_2();$
\State ...
\State $task\_n();$
\State $task\_2();$
\}

\end{algorithmic}
\end{algorithm}

Für die Handhabung von Aufgaben kann, im Gegensatz zu Events ein zyklischer Ansatz verwendet werden. Deren Frequenz kann bestimmt und somit fest implementiert werden, was sie von Events unterscheidet, die in der Regel azyklisch auftreten und eine Flag setzen. Somit werden hier innerhalb eines Durchlaufs alle Aufgaben sequentiell behandelt.\\
Um Aufgaben mit einer hohen Auftrittsfrequenz besondere Beachtung schenken zu können wird diese Aufgabe innerhalb jedes Durchlaufes mehrmals ausgeführt, im Beispiel \ref{sced2} wäre dies $task\_2$. Sobald jedoch ein schwierigeres Problem vorliegt, so sollte dieser Ansatz nicht gewählt werden. Die 'faire' Verteilung der Aufgaben innerhalb der Schleife, um alle Deadlines erfüllen zu können wird zu einer nicht trivialen Aufgabe. Zudem wird die Länge des Codes der Schleife stark ansteigen, sollten mehrere Aufgaben öfters aufgerufen werden.

\subsubsection{Coroutines}

\begin{algorithm}
\captionsource{Coroutine}{\cite[p. 86]{RTS}}
\label{sced3}
\begin{algorithmic}[1]

\BState \textit{void $task\_a$(void)}
\{
\BState \textit{While(True)}\\
\{
\State $switch(state\_a)$
\{
\State case 1: $phase\_a1();$
\State $break;$
\State case 2: $phase\_a2();$
\State $break;$
\State case 3: $phase\_a3();$
\State $break;$
\}\\
\}\\
\}
\newline
\BState \textit{void $task\_b$(void)}
\{
\BState \textit{While(True)}\\
\{
\State $switch(state\_b)$
\{
\State case 1: $phase\_b1();$
\State $break;$
\State case 2: $phase\_b2();$
\State $break;$
\State case 3: $phase\_b3();$
\State $break;$
\}\\
\}\\
\}


\end{algorithmic}
\end{algorithm}

Als Koroutinen werden die in \ref{sced3} erwähnten $task\_a$ und $task\_b$ bezeichnet. Der Dispatcher ruft nacheinander die einzelnen Tasks auf, welche jeweils in Abschnitte unterteilt sind. So wird nach jeder ausgeführten Phase der zugehörige globale Zähler inkrementiert und anschließend die Kontrolle dem Dispatcher zurück gegeben. Anschließend kann die nächste Aufgabe aufgerufen werden. Bei erneutem Aufruf einer AUfgabe wird der jeweils nächste Abschnitt ausgeführt, da der Zähler nun auf einen anderen Zustand hinweist.\\
Somit lässt sich pseudo-parallele Ausführung mehrerer Aufgaben realisieren. Durch die Unterteilung in einzelne Abschnitte sind Aufgaben zudem in der Lage, Interrupts besser behandeln zu können.
\newpage
\subsubsection{Interrupt-Only System}
\begin{algorithm}
\captionsource{Interrupt-Only}{\cite[p. 89]{RTS}}
\label{sced4}
\begin{algorithmic}[1]

\BState \textit{void $main$(void)}
\{
\State $init();$
\State $while(True);$\\
\}\\

\BState \textit{void $int\_1$(void)}
\{
\State $save(context);$
\State $task\_1();$
\State $restore(context)$;\\
\}\\

\BState \textit{void $int\_2$(void)}
\{
\State $save(context);$
\State $task\_2();$
\State $restore(context)$;\\
\}\\

\BState \textit{void $int\_3$(void)}
\{
\State $save(context);$
\State $task\_3();$
\State $restore(context)$;\\
\}

\end{algorithmic}
\end{algorithm}
In diesem Beispiel ruft sich das $main$-Programm mittels jump-to-self immer wieder selber auf. Aufgaben werden lediglich mittels Interrupts angekündigt. Der aktuelle Kontext wird gespeichert, die Aufgabe behandelt und der Kontext anschließend geladen. In diesem Beispiel werden alle Interrupts durch die Hardware ausgelöst.\\

\subsection{Erweitertes Scheduling}\label{2.7}
In fortgeschrittenen System werden höhere Anforderungen an die Verteilung von Aufgaben gestellt. Diese muss dynamisch und unter Berücksichtung der Frequenz, der Priorität und der Laufzeit stattfinden. Dazu wird im Folgenden zunächst die Grundlage dazu geliefert, und anschließend werden übliche Realisierungen vorgestellt.

\subsubsection{Grundlegende Begriffe}
\begin{definition}{Priorität}\\
Die Priorrität ist eine Eigenschaft, die einer Aufgabe zugeordnet wird und in der Regel unveränderlich ist. Aufgaben können solche geringerer Priorität unterbrechen und zuerst ausgeführt werden, wenn diese initialisiert werden.
\end{definition}
Sollte diese Eigenschaft für ein System genutzt werden, so ist es wichtig, Prioritäten sorgsam zu wählen. Jede Unterbrechung erzeugt einen weiteren Delay, hervorgerufen durch das Speichern und Laden des aktuellen Zustandes. Zudem sorgt dies für Nichtdeterminismus.
\begin{definition}{Zeitfenster}\\
Ein Zeitfenster ist ein einer Aufgabe zugeordnete Zeitspanne, in welcher diese ausgeführt wird.
\end{definition}
Wird solch ein Zeitfenster genutzt, die Aufgabe jedoch nicht in der vorgegebenen Zeitspanne beendet, so wird die Aufgabe unterbrochen, der aktuelle Inhalt der Variablen gespeichert und bei erneuter Ausführung aufgerufen.\\
Viele Systeme sind so erstellt, dass die Zeitfenster nur ein Bruchteil der eigentlichen Laufzeit ausmachen. Somit wird eine Aufgabe mehrmals unterbrochen, ehe diese fertig ist. Somit kann zu jedem Zeitpunkt schnell auf Interrupts reagiert werden, da einige Systeme die Unterbrechung von einer Aufgabe nicht zulassen, sondern die mit der Unterbrechung verknüpfte Aufgabe erst nach dem aktuell Zeitfenster ausführen.
\begin{definition}{TCB}\\
Ein TCB('Task Control Block') ist eine Datenstruktur, welche einer Aufgabe zugewiesen wird. In dieser werden unter anderem Priorität, Status, zugeordnere Register, Programmzähler sowie ein Pointer auf den nächsten TCB gespeichert.
\end{definition}
TCBs eignen sich bei einem System, deren Aufgaben mit Eingabevariablen, Programmcountern sowie Interrupts einhergehen. So können Ergebnisse zwischengespeichert werden, und Interrupts können sofort ohne Verluste behandelt werden.


\begin{definition}{Zustand}\\
Der Zustand einer Aufgabe gibt an, ob diese gerade oder zukünftig ausgeführt wird, oder ob deren Ausführung bereits ansteht. Dabei gibt es die Zustände 'Bereit', 'Wird ausgeführt', 'Unterbrochen' sowie 'Inaktiv'.
\end{definition}
Eine Aufgabe mit dem Zustand 'Bereit' steht für möglichst baldige Ausführung an. Dies setzt voraus, dass alle Bedingungen für die Ausführung erfüllt sind.\\
Befindet sich eine Aufgabe in dem Zustand 'Wird ausgeführt', so ist dies die aktuelle Aufgabe des Prozessors. Eine Aufgabe kann diesen Zustand nur erreichen, wenn sie zuvor 'Bereit' war, oder wenn sie gerade angelegt worden ist und keine andere Aufgabe behandelt wird.\\
Nach erfolgreicher Ausführung wandert eine Aufgabe in den Zustand 'Unterbrochen'. Sie wird erst wieder durch einen entsprechenden Befehl auf den Zustand 'Bereit' gesetzt.\\
Wird eine Aufgabe jedoch während der Ausführung unterbrochen, da ihre maximale Laufzeit erreicht ist, so wird der Zustand erneut auf 'Bereit' gesetzt, und die aktuellen Variablen werden gespeichert, um mit der Ausführung fortfahren zu können.\\
Der verbleibende Zustand 'inaktiv' wird nur dann genutzt, wenn die Anzahl an TCBs festgesetzt ist. Diese Aufgaben existieren, können jedoch aktuell nicht ausgeführt werden.

\subsubsection{Round-Robin Scheduling}
Round-Robin Scheduling bietet die einfachste Form eines Systems, welches mehrere in Segmente unterteilte Aufgaben behandelt. Dies geschieht, indem ein Zeitfenster $t_\delta$ bestimmt wird. Dieses wird in einem zyklischen Verfahren jeder Aufgabe zugeordnet, welche in diesem Zeitfenster behandelt wird. Nach Ablauf der Zeit oder Beendigung der Aufgabe wird diese in den Hintergrund geschoben, um die nächste Aufgabe abzuarbeiten. So kann das System zu jedem Zeitpunkt nach spätestens $t_\delta$ unterbrochen werden, was zu einer guten Reaktionszeit für Interrupts führt.\\
Zu einer verbesserten Handhabung lassen sich auch hier Prioritäten bestimmten, sodass eine Aufgabe höherer Priorität zu jedem beliebigen Zeitpunkt eintreten und alle Tasks unterliegender Priorität unterbrechen kann, während deren jeweiliges Zeitfenster weiter abläuft. Dies führt wiederum zu einer komplizierteren Wartung. Beispielsweise ist es möglich, dass eine Aufgabe hoher Priorität immer die Aufgabe $n_i$ unterbricht, so dass diese niemals behandelt wird.

\subsubsection{Cyclic Code Scheduling}
\label{2.7.3}
Ein Verfahren ähnlich dem Round-Robin Scheduling ist Cyclic Code Scheduling. Hier wird ein Zeitfenster $t_\delta$ bestimmt, welcher ähnlich des Round-Robin Verfahrens abwechselnd allen Aufgaben zugeteilt wird. Hier gelten jedoch einschränkungen an dieses Zeitfenster.
\begin{itemize}
\item $t_\delta \geq max (e_i)$
\item $\lfloor p_{hyper} / t_\delta\rfloor - p_{hyper} / t_\delta = 0$
\item $2 t_\delta - gcd(p_i, t_\delta) \leq D_i$
\end{itemize}
wobei
\begin{itemize}
\item $p_i$ die minimale Zeit zwischen zwei Beendigungen der Aufgabe $n_i$ ist
\item $e_i$ die Ausführungszeit der Aufgabe $n_i$ ist
\item $D_i$ die maximale Zeit zwischen Aktivierung und Beendigung der Aufgabe $n_i$ ist.
\item $p_{hyper}$ das kleinste gemeinsame Vielfache aller $p_i$ ist
\item $gcd(a,b)$ den kleinsten gemeinsamen Teiler von a und b darstellen.
\end{itemize}
Dabei ist der größte Unterschied zu Round-Robin neben diesen Bedingungen die Tatsache, dass Aufgaben nicht von solchen höherer Priorität unterbrochen werden können. Die Zeitfenster sind im Vorfeld so angepasst, dass alle Aufgaben ihre Deadline (in der Regel relativ knapp) einhalten können, somit sind Änderungen nach dem Beginn eines Zyklus nicht möglich.\\
Deswegen wird der genaue Ablauf zu Beginn jedes Zyklus neu berechnet, um aperiodisch auftretende Aufgaben berücksichtigen zu können.

\subsubsection{Fixed-Priority Scheduling: Rate-Monotonic Approach}
\label{2.7.5}
Bei diesem Ansatz wird jeder Aufgabe eine Priorität entsprechend ihrer Periode $p_i$ zugeteilt. Dabei ist zu beachten, dass Aufgaben niedrigerer Periodendauer eine höhere Priorität erhalten. Wie in \cite[p. 102f]{RTS} bewiesen, resultiert dieser Ansatz in der optimalen Verteilung der Aufgaben. Ebenfalls wird eine obere Grenze des 'utilization factors' in Abhängigkeit der ANzahl an Aufgaben gezeigt.\\
\begin{definition}{Utilization factor}\\
Der 'Utilization factor' (U) ergibt sich über die Summe der Quotienten der Laufzeit und Periodendauer der einzelnen Tasks, also:\\
$U = \sum\limits_{i=1}^n \frac{e_i}{p_i}$\\
\end{definition}
Dabei gibt dieser Wert $U$ einen Prozentsatz der Zeit an, welcher von Aufgaben genutzt wird. Dabei enntspräche $U=0.87$ einer Auslastung der CPU von 87\% bei optimaler Aufteilung der Aufgaben.\\
Für dieses Verfahren existiert die Grenze $n(2^{1/n} - 1)$. Liegt der Wert $U$ für $n$ periodische Aufgaben über diesem Wert, so ist nicht garantiert, dass Fixed-Priority Scheduling eine Lösung bietet, bei der keine Deadlines verletzt werden. Unterhalb dieser Grenze jedoch funktioniert das Verfahren in jedem Fall.

\subsubsection{Dynamic Priority Scheduling: Earliest Deadline First Approach}
Wie der Name suggeriert, so wird hier zu jedem Zeitpunkt die Aufgabe behandelt, deren Deadline unmittelbar bevorsteht. Wird eine Aufgabe aktiviert, so wird geprüft, ob deren Deadline vor der der aktuellen Aufgabe liegt. Ist dies der Fall, so wird die aktuelle Aufgabe unterbrochen und stattdessen die neue behandelt.\\
Dieses Schedulingverfahren wird zu Laufzeit des Systems ausgeführt, nicht zu Kompilierzeit, was bei den anderen vorgestellten Verfahren der Fall war.\\
Dank dieser Verteilung der Aufgaben ist es möglich, ein System mit einem Wert von $U \leq 1$ zu realisieren und zu verhindern, dass Deadlines verpasst werden. Hier liegt der größte Vorteil gegenüber dem in \ref{2.7.5} erläuterten Verfahren.\\
Der Nachteil liegt in der Dynamik, mit der die Aufgaben behandelt werden. Fixed-Priority Scheduling ist berechenbar, da die Priorität der Aufgaben statisch und abhängig von deren Periodendauer ist. Somit wird mit einem dynamischen System Nichtdeterminismus eingeführt. Zudem ist ein statisches Verfahren robuster gegenüber verpassten Deadlines. Wird bei dem in diesem Kapitel eingeführten Verfahren eine Deadlines verpasst, so führt dies in der Regel zu weiteren verpassten Deadlines und schließlich zu einem Ausfall des Systems. Bei dem Verfahren in \ref{2.7.5} würde diese Aufgabe ihre Deadline in einem periodischen Zyklus verpassen, die restlichen Aufgaben würden jedoch weiterhin rechtzeitig ausgeführt werden können.

\subsubsection{Anwendung bei einem DSP}
Bei einem 'digital signal processor' sind in der Regel sowohl Aufgaben sowie deren Ausführungs- und Periodendauer bekannt. Somit lässt sich solch ein System mithilfe eines statischen Scheduling Verfahrens realisieren, wodurch overhead sowie nicht-determinismus in diesem Teil eliminiert werden\cite{Scheduling}. Die Unterbrechung anderer Aufgaben ist in einem DSP in der Regel nicht möglich, wodurch sich beispielsweise das in \ref{2.7.3} vorgestellte Verfahren anbietet.



%\begin{definition}{}
%\end{definition}
%\subsection{System Service}\label{2.8}